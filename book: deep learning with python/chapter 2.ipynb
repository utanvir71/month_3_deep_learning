{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a7ef82",
   "metadata": {},
   "source": [
    "**<h1>THE MATHEMATICAL BUILDING BLOCKS OF NEURAL NETWORKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4a613",
   "metadata": {},
   "source": [
    "# *A FIRST LOOK AT A NEURAL NETWORK*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e3815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "import torch\n",
    "from torch import nn\n",
    "# selecting device\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7f3fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x118394b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproduction\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e7ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081)) # MNIST stats\n",
    "])\n",
    "\n",
    "# Location of data\n",
    "DATA_DIR = 'data'\n",
    "train_ds = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transforms)\n",
    "test_ds = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                          num_workers=2, pin_memory=(device.type=='cuda'))\n",
    "test_loader = DataLoader(test_ds, batch_size=1024, shuffle=False,\n",
    "                         num_workers=2, pin_memory=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70688725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from torch import nn\n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = MNIST_CNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6aa8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimezer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6728b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train)\n",
    "        loss = criterion(preds, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X_train.size(0)\n",
    "        pred = preds.argmax(1)\n",
    "        correct += (pred == y_train).sum().item()\n",
    "        total += X_train.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "@ torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05667aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss 0.1698 acc 0.9464 | test loss 0.0473340841203928 acc 0.9838\n",
      "Epoch 01 | train loss 0.0576 acc 0.9826 | test loss 0.03359404293298721 acc 0.9884\n",
      "Epoch 02 | train loss 0.0426 acc 0.9868 | test loss 0.029615993732213974 acc 0.9896\n",
      "Epoch 03 | train loss 0.0334 acc 0.9894 | test loss 0.033102848029136656 acc 0.9904\n",
      "Epoch 04 | train loss 0.0277 acc 0.9908 | test loss 0.028795895338058472 acc 0.9914\n",
      "Epoch 05 | train loss 0.0227 acc 0.9926 | test loss 0.02116601152420044 acc 0.9937\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_epoch()\n",
    "    te_loss, te_acc = evaluate()\n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} \"\n",
    "          f\"| test loss {te_loss} acc {te_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cbf185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2cc785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4383148",
   "metadata": {},
   "source": [
    "## 2.2 Data representation for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657dba32",
   "metadata": {},
   "source": [
    "### 2.2.1 Scaler (rank-0 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3837f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3386d",
   "metadata": {},
   "source": [
    "### 2.2.2 Vectors (rank-1 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "657398e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12,  3,  6, 14,  7]), 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([12,3,6,14,7])\n",
    "x, x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47158f11",
   "metadata": {},
   "source": [
    "### Matrics (rank02 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7bda937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[5, 78,2, 35, 0],\n",
    "              [6, 779, 3, 25, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f154c",
   "metadata": {},
   "source": [
    "### Rank-3 and higher-rank tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b948983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "            [6, 79, 3, 35, 1],\n",
    "            [7, 80, 4, 36, 2]],\n",
    "            [[5, 78, 2, 34, 0],\n",
    "            [6, 79, 3, 35, 1],\n",
    "            [7, 80, 4, 36, 2]],\n",
    "            [[5, 78, 2, 34, 0],\n",
    "            [6, 79, 3, 35, 1],\n",
    "            [7, 80, 4, 36, 2]]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d550742",
   "metadata": {},
   "source": [
    "### 2.2.5 Key atrributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f5509",
   "metadata": {},
   "source": [
    "A tensor is defined by threee key attritutes:\n",
    "* Number of axes *(rank)*--For insttance,a rank-3 tensor has three axes, and a matrix as two axes. This is also called the tensor's ndim in python libraries such as NumPy or TensorFlow or PyTorch.\n",
    "* Shape--Thes is a tuple of integers that describes how many dimensions the tensor has algong each axis. For instance, the previous matrix example has shape (3, 5), adn the rank-3 tensor example has shape(3,3,5), A vector has shape with a single element, such as (5,), whereas a scalar has an empty shape, ().\n",
    "* *Data type (usually called **dtype** in python libaries)* -- This is the type of the data ontained in the tensor; for instance, a tensor's type could be float16, float32, float 64, uint8,, and so on. In tensorFlow, you are also likely to come across string tensors.\n",
    "\n",
    "\n",
    "To make this more concrete. lets look back at the data we proccesed in MNIST example. First, we load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b03c50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db90829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)  # Happens if GPU was already initialized\n",
    "\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52515fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import test_util\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae26053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply():\n",
    "    x = tf.random.uniform((1000, 1000), dtype=tf.float32)\n",
    "    y = tf.random.uniform((1000, 1000), dtype=tf.float32)\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73973e69",
   "metadata": {},
   "source": [
    "### 2.2.6 Manipulatng tensors in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b54a3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 23:25:14.839190: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-08 23:25:14.839380: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-09-08 23:25:14.839386: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-09-08 23:25:14.839756: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-08 23:25:14.839773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 μs ± 271 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "with tf.device(\"/GPU:0\"):\n",
    "     multiply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ae372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    x = tf.random.uniform((10000, 10000), dtype=tf.float32)\n",
    "    y = tf.random.uniform((10000, 10000), dtype=tf.float32)\n",
    "    c = tf.matmul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c763ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x\n",
    "del y\n",
    "del c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32c976b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Memory growth warning:\", e)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefa1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2048, 2048])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.random.normal([2048, 2048])   # ~16 MB\n",
    "    b = tf.random.normal([2048, 2048])\n",
    "    c = tf.matmul(a, b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ad328",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a\n",
    "del b\n",
    "del c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c709c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.9 ms ± 775 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.random.normal([4096, 4096])   # now feasible\n",
    "    b = tf.random.normal([4096, 4096])\n",
    "    c = tf.matmul(a, b)                  # result is float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a\n",
    "del b\n",
    "del c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.48 s ± 199 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    a = tf.random.normal([10000, 10000])\n",
    "    b = tf.random.normal([10000, 10000])\n",
    "    c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a\n",
    "del b\n",
    "del c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371b8d9",
   "metadata": {},
   "source": [
    "## Back to book 2.2.4\n",
    "### By tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6f32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dc0430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154e670c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5302c1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, (60000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.ndim, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978eccb",
   "metadata": {},
   "source": [
    "### by pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a202a5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "DATA_DIR = 'data'\n",
    "train_ds = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transforms.ToTensor)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d24a7",
   "metadata": {},
   "source": [
    "**Note**: The train_ds is not a tensor. its a ***object*** that has two tensor inside:\n",
    "* *data*\n",
    "* *target*\n",
    "\n",
    "So train_ds work like a  wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35333c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fe1427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf17182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([60000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.targets.ndim, train_ds.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81f3fd",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf35e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba45de",
   "metadata": {},
   "source": [
    "### Torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58af523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_ds.data[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f480f7",
   "metadata": {},
   "source": [
    "## 2.2.6 Manupulating tensors in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0e61d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow\n",
    "my_slice = train_images[10:100]\n",
    "display(my_slice.shape)\n",
    "# torch\n",
    "my_slice_pt = train_ds.data[10:100]\n",
    "my_slice_pt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481226ed",
   "metadata": {},
   "source": [
    "### The notion of data batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45036b",
   "metadata": {},
   "source": [
    "In general, the first axis (axis 0, bacause indexing starts at 0) in all data sensors you will com across in deep learning will be the samples axis (sometimes called the samples dimension). In the MNIST example, samples are images of digits.\n",
    "\n",
    "In addition, deep learniing models don't process an entire dataset at onece; rather they break the data into small batches, Concretely. here's one batch of our MNIST digits, with a batch size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970625f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---tensorflow---\n",
    "batch = train_images[:128]\n",
    "## then next batch\n",
    "batch = train_images[128:256]\n",
    "## and The nth batch:\n",
    "n = 3\n",
    "batch = train_images[128 * n:128*(n+1)]\n",
    "#---torch---\n",
    "batch = train_ds.data[:128]\n",
    "## tehen next batch\n",
    "batch = train_ds.data[128:256]\n",
    "## and the nth batch\n",
    "n = 3\n",
    "batch = train_ds.data[128*n : 128*(n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0142",
   "metadata": {},
   "source": [
    "## 2.2.8 Real-world example of data tensros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bd47e",
   "metadata": {},
   "source": [
    "The data you'll manipulate will almost always fall into following catagories: \n",
    "* ***vector data** *: Rank-2 tensors of shape (sample, fetures), where eatch sample is a vector of numerical attributes (\"features\")\n",
    "\n",
    "\n",
    "* ***Timeseries data*** : Rank-3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of freature vectors\n",
    "\n",
    "\n",
    "* ***Images*** : Rank-4 tensors of shape (sample, height, width, channels), where each samples is a 2D grid of pixels, and each pixel is represented by a vector of vlues (\"Channels\")\n",
    "\n",
    "\n",
    "* ***Video*** : Rank-5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805a731",
   "metadata": {},
   "source": [
    "## 2.2.9 Vector data\n",
    "This is one of the most common cases. In such a dataset, each single data point can be\n",
    "encoded as a vector, and thus a batch of data will be encoded as a rank-2 tensor (that\n",
    "is, an array of vectors), where the first axis is the samples axis and the second axis is the\n",
    "features axis.\n",
    "\n",
    "Let's take at two examples:\n",
    "* An actuarial dataset of people, where we consider each person’s age, gender,\n",
    "and income. Each person can be characterized as a vector of 3 values, and thus\n",
    "an entire dataset of 100,000 people can be stored in a rank-2 tensor of shape\n",
    "(100000, 3).\n",
    "\n",
    "* A dataset of text documents, where we represent each document by the counts\n",
    "of how many times each word appears in it (out of a dictionary of 20,000 common\n",
    "words). Each document can be encoded as a vector of 20,000 values (one\n",
    "count per word in the dictionary), and thus an entire dataset of 500 documents\n",
    "can be stored in a tensor of shape (500, 20000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460283a5",
   "metadata": {},
   "source": [
    "## 2.2.10 Timeseries data of sequence data\n",
    "Whenever time matters in your data (or the notion of sequence order), it makes sense\n",
    "to store it in a rank-3 tensor with an explicit time axis. Each sample can be encoded as\n",
    "a sequence of vectors (a rank-2 tensor), and thus a batch of data will be encoded as a\n",
    "rank-3 tensor.\n",
    "\n",
    "\n",
    "![sample image](images/1.png)\n",
    "\n",
    "\n",
    "<!-- <img src=\"images/1.png\" weight=\"300\"/> -->\n",
    "The time axis is always the second axis (axis of index 1) by convention. Let's look at a few examples:\n",
    "* A dataset of stock prices. Every minute. we store the current price of the stock the highest price in the past minute, and the lowest price in the past minute. Thus, every minute is encuded as 3D vector, as etire day of trading is ecoded as a matrix as a matrix of shape (390, 3) (there are 390 minutes in the trading data), and 250 days' worth of data can be stored in a rank-3 tensor of shape(250, 390,3). Here each sample would be one day's wordth of data\n",
    "* A dataset of tweets, were we encode each tweet as a sequence if 289 char out of an alphabet of 128 unique chars. In this setting, each character can be encoded as a binary vector of size 1(an all-zeroes vector exept for a 1 entry at the index correspondeing to the chacter). Then each tweet can be encoded as a rank-2 tensor of shape (289, 128). and a dataset of 1 million tweets can be encoded as a randk-2 tensor of shape (280, 128), and a dataset of 1 million tweets can be stored in a tensor of shape (1,000,000, 280, 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965592a",
   "metadata": {},
   "source": [
    "### 2.2.11 Image data\n",
    "Image typically have three dimensions: height, width and color depth. Although grayscale images (like our MNIST digts) have only a single color chanell and could thus be stored in randk 2 tensor. by convertion image tensors are always rank-3, with a one dimensional color cchannel for grayscale images. Ab atch of 128 grauscale images of size 256 x 256 coud thus be stored in a tensor of shape (128, 256, 256, 1), and a batch of 138 color images could be stored in a tensor of shape  (128, 256, 256, 3)\n",
    "\n",
    "\n",
    "![blablabla](images/2.png)\n",
    "\n",
    "***Note***:\n",
    "* Tensorflow standard: ```(sample, height, width, channels)```\n",
    "* Pytorch standard: ```(sameple, channels, height, width)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d0cf1",
   "metadata": {},
   "source": [
    "### 2.2.12 Video data\n",
    "Video data is one of the few types of real-world data for which you’ll need rank-5 tensors.\n",
    "A video can be understood as a sequence of frames, each frame being a color\n",
    "image. Because each frame can be stored in a rank-3 tensor (height, width, color_\n",
    "depth), a sequence of frames can be stored in a rank-4 tensor (frames, height,\n",
    "width, color_depth), and thus a batch of different videos can be stored in a rank-5\n",
    "tensor of shape (samples, frames, height, width, color_depth).\n",
    "For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\n",
    "second would have 240 frames. A batch of four such video clips would be stored in a\n",
    "tensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the dtype of the tensor was float32, each value would be stored in 32 bits, so the tensor\n",
    "would represent 405 MB. Heavy! Videos you encounter in real life are much lighter,\n",
    "because they aren’t stored in float32, and they’re typically compressed by a large factor\n",
    "(such as in the MPEG format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9fefe",
   "metadata": {},
   "source": [
    "## 2.3 The gears of nerual networks: Tensor operations\n",
    "A keras layer instance look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ca4742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense, built=False>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "keras.layers.Dense (512, activation='relu')\n",
    "# output = relu(dot(input, W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6535f6a",
   "metadata": {},
   "source": [
    "In PyTorch dense is **nn.Linear** and you add **ReLU** separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9d16415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# Example input has 32 batch of 1000 feature\n",
    "x = torch.rand(32, 1000)\n",
    "# Dense laryer 1000->512\n",
    "layer = nn.Linear(1000, 512)\n",
    "relu = nn.ReLU()\n",
    "\n",
    "output = relu(layer(x))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a048c25",
   "metadata": {},
   "source": [
    "🧠 What actually happens\n",
    "\n",
    "\n",
    "If input is shaped [batch_size, 1000]:\n",
    "*\tMultiply by weight matrix W of shape [1000, 512].\n",
    "*\tAdd bias vector b of shape [512].\n",
    "*\tApply activation (e.g. ReLU).\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "\\text{output} = \\text{ReLU}(XW + b)\n",
    "*\tInput shape → [batch, 1000]\n",
    "*\tOutput shape → [batch, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c765c0",
   "metadata": {},
   "source": [
    "### 2.3.1 Element-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae18645",
   "metadata": {},
   "source": [
    "native python way relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "518bafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def native_relu(x):\n",
    "    assert len(x.shape) == 2 # x is a rank-2 NumPy tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i,j],0)\n",
    "    return x\n",
    "\n",
    "def native_add(x,y):\n",
    "    assert len(x.shape)==2\n",
    "    assert x.shape==y.shape\n",
    "    x = x.copy()\n",
    "    for i in range (x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]+=y[i,j]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974b2cb",
   "metadata": {},
   "source": [
    "### 2.3.2 BroadCasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a616ed7",
   "metadata": {},
   "source": [
    "### 2.3.3 Tensor Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff9478",
   "metadata": {},
   "source": [
    "### 2.3.4 Tensor reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffb2a8",
   "metadata": {},
   "source": [
    "### 2.3.6 Visual representation of tensor manupulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616d179",
   "metadata": {},
   "source": [
    "## 2.4 *(very Important)* ***The engine of neural networkds: Gradient-based optimization***\n",
    "1. Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "2. Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "3. Compute the loss of the model on the batch, a measure of the mismatch between\n",
    "y_pred and y_true.\n",
    "4. Update all weights of the model in a way that slightly reduces the loss on this\n",
    "batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12881c10",
   "metadata": {},
   "source": [
    "### THE GRADIENT TAPE in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91816d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 23:32:29.419375: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-08 23:32:29.419571: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-09-08 23:32:29.419583: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-09-08 23:32:29.420035: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-08 23:32:29.420055: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(0.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "grad_of_wrt_x = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f79f3",
   "metadata": {},
   "source": [
    "### Pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "388d5636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.0, 2.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(0., requires_grad=True)\n",
    "y = 2 * x + 3\n",
    "y.backward()\n",
    "x.item(), y.item(), x.grad.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b17122",
   "metadata": {},
   "source": [
    "Key differences Pytorch vs TensorFlow GradientTape:\n",
    "*\tNo tape context needed — PyTorch builds the computational graph on the fly.\n",
    "* Call .backward() on the output scalar → gradients flow back automatically.\n",
    "* Gradients are stored in x.grad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2e484",
   "metadata": {},
   "source": [
    "# 2.5 **Looking at our first example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008929a1",
   "metadata": {},
   "source": [
    "This was the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c36d4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF code\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "test_images = test_images.reshape(10000, 28*28)\n",
    "test_images = test_images.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch code\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1) Define transform: flatten 28x28 → 784 and normalize 0-255 → 0-1\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # shape: [1, 28, 28], values in [0,1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))     # flatten to [784]\n",
    "])\n",
    "\n",
    "# 2) Download datasets\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# 3) Data loaders (like batching in Keras)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Example: grab a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # torch.Size([128, 784])\n",
    "print(labels.shape)  # torch.Size([128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e70bb",
   "metadata": {},
   "source": [
    "This was our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2178f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanvir15/Documents/12 month ai challange/deep_learning/.venv_tf/lib/python3.11/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_8, built=True>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF code:\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model_1 = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\", input_shape=(784,)),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc3d48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# PT code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(784, 512), #input size, hidden size\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10), # hidden size, output size\n",
    "    # nn.Softmax(dim=1)\n",
    ")\n",
    "model2 = model2.to(device)\n",
    "device = 'mps'\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57061b5d",
   "metadata": {},
   "source": [
    "Now you understand that this model consists of a chain of two Dense layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. Weight tensors, which are attributes of the layers, are where the knowledge of the model persists.\n",
    "\n",
    "  This was the model-compilation step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9d4bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf code\n",
    "model_1.compile(optimizer=\"rmsprop\", \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00550f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTroch code\n",
    "import torch\n",
    "# optimizer\n",
    "optimizer = torch.optim.RMSprop(model2.parameters())\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# metrics (have to make manually)\n",
    "def accuracy(y_pred, y_true):\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    # torch.argmax: \n",
    "\t# It returns the index of the maximum value along a given dimension.\n",
    "\t# Think of it as: “which class has the highest score?”\n",
    "    return (y_pred == y_true).float().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb63070",
   "metadata": {},
   "source": [
    "Now you understand that sparse_categorical_crossentropy is a loss function that's used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize. You also know that this reduction of the loss happens via mini-batch stochastic gradient descent. The exact rules governing a specific use of gradient descent are defined by the rmsprop optimizer passed as the first argument.\n",
    "\n",
    "Finally this was the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d479faba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8879 - loss: 0.3885\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9115 - loss: 0.3177\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9112 - loss: 0.3200\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9113 - loss: 0.3258\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9086 - loss: 0.3378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1695916d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF code\n",
    "model_1.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a95ff0",
   "metadata": {},
   "source": [
    "Here comes the good part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83f5c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f530c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train_loss: 0.9274 | train_acc: 0.9196\n",
      "Epoch 2/5 | train_loss: 0.1286 | train_acc: 0.9615\n",
      "Epoch 3/5 | train_loss: 0.1032 | train_acc: 0.9699\n",
      "Epoch 4/5 | train_loss: 0.0827 | train_acc: 0.9765\n",
      "Epoch 5/5 | train_loss: 0.0744 | train_acc: 0.9789\n"
     ]
    }
   ],
   "source": [
    "# PyTorch code of model.fit\n",
    "# there is nothing called model.fit in PyTorch \n",
    "# Training:\n",
    "def trian_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss, running_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in loader: \n",
    "        x, y = x.to(device), y.to(device) # here device is gpu/cpu\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output=model(x)\n",
    "        loss=criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        b=x.size(0)\n",
    "        preds=torch.argmax(output, dim=1)\n",
    "        running_loss += loss.item() * b\n",
    "        running_acc  += (preds == y).float().sum().item()\n",
    "        n+=b\n",
    "    return running_loss/n, running_acc/n\n",
    "\n",
    "# Now we can make the main training loop like model.fit\n",
    "epochs=5\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = trian_one_epoch(model2, train_loader, optimizer, criterion)\n",
    "    # val_loss, val_acc = evaluate(model2, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch}/{epochs} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f}\")\n",
    "        #   f\"val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846b640",
   "metadata": {},
   "source": [
    "### 2.5.1 Reimplementing our first example from scratch in TensorFlow and PyTorch\n",
    "#### **TENSORFLOW CODE**\n",
    "<h5>A SIMPLE DENSE CLASS</h5>\n",
    "I've learned earlier that the Dense layer implements the following input transformation, where W and b are model parameters, and activation is an element-wise function:\n",
    "* output = activation(doe(w,input)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96338ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.w = tf.Variable(w_initial_value)\n",
    "        b_shape = (output_size,)\n",
    "        b_inital_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_inital_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(input, self.w) + self.b)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.w, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc42315",
   "metadata": {},
   "source": [
    "<h5>A SIMPLE SEQUENTIAL CLASS</H5>\n",
    "Now, let's create a Naive Seqential class to chain these layers. It Wraps a list of layers and exposes a __call__() method that simply calls the underlying layers on the inputs inputs, in order. It also features a weights property to easily keep track of the layers' parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6406a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b975c63",
   "metadata": {},
   "source": [
    "Using the NaiveDense class and NaiveSequential class, we can create a mock Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4d2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30326b",
   "metadata": {},
   "source": [
    "<h5>A BATCH GENERATIOR</H5>\n",
    "Next, we need a way to iterate over the MNIST data in mini batches. This is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87607aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images)/batch_size)\n",
    "    \n",
    "    def next (self):\n",
    "        images = self.images[self.index:self.index+self.batch_size]\n",
    "        labels = self.labels[self.index:self.index+self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246862ff",
   "metadata": {},
   "source": [
    "### 2.5.2 RUNNNING ONE TRAINING SETP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cefe6b",
   "metadata": {},
   "source": [
    "*I am not rewriting this in pytorch because its just for clearing the concept and i don't want to waste too much time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f76a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def one_training_setp(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losess.spare_categorical_crossentropy(\n",
    "            labels_batch, predictions\n",
    "        )\n",
    "        avarage_loss = tf.reduce_mean(per_sample_losses)\n",
    "        gradients = tape.gradient(avarage_loss, model.weights)\n",
    "        update_weights(gradients, model.weights)\n",
    "        return avarage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6783bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(gradients, weights):\n",
    "    for g, w in  zip(gradients, weights):\n",
    "        w.assign_sub(g*learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288a69c",
   "metadata": {},
   "source": [
    "### 2.5.3 THE FULLL TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb154f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "    batch_generator = BatchGenerator(images, labels)\n",
    "    for batch_counter in range(batch_generator.num_batches):\n",
    "        images_batch, labesl_batch = batch_generator.next()\n",
    "        loss = one_training_setp(model, images_batch, labels_batch)\n",
    "        if batch_counter %100 == 0:\n",
    "            print(f\"Loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8969d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a7ef82",
   "metadata": {},
   "source": [
    "**<h1>THE MATHEMATICAL BUILDING BLOCKS OF NEURAL NETWORKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4a613",
   "metadata": {},
   "source": [
    "# *A FIRST LOOK AT A NEURAL NETWORK*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e3815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "import torch\n",
    "from torch import nn\n",
    "# selecting device\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7f3fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a23b4f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproduction\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e7ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081)) # MNIST stats\n",
    "])\n",
    "\n",
    "# Location of data\n",
    "DATA_DIR = 'data'\n",
    "train_ds = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transforms)\n",
    "test_ds = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                          num_workers=2, pin_memory=(device.type=='cuda'))\n",
    "test_loader = DataLoader(test_ds, batch_size=1024, shuffle=False,\n",
    "                         num_workers=2, pin_memory=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70688725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from torch import nn\n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = MNIST_CNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6aa8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimezer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6728b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train)\n",
    "        loss = criterion(preds, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X_train.size(0)\n",
    "        pred = preds.argmax(1)\n",
    "        correct += (pred == y_train).sum().item()\n",
    "        total += X_train.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "@ torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05667aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss 0.1698 acc 0.9464 | test loss 0.0473340841203928 acc 0.9838\n",
      "Epoch 01 | train loss 0.0576 acc 0.9826 | test loss 0.03359404293298721 acc 0.9884\n",
      "Epoch 02 | train loss 0.0426 acc 0.9868 | test loss 0.029615993732213974 acc 0.9896\n",
      "Epoch 03 | train loss 0.0334 acc 0.9894 | test loss 0.033102848029136656 acc 0.9904\n",
      "Epoch 04 | train loss 0.0277 acc 0.9908 | test loss 0.028795895338058472 acc 0.9914\n",
      "Epoch 05 | train loss 0.0227 acc 0.9926 | test loss 0.02116601152420044 acc 0.9937\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_epoch()\n",
    "    te_loss, te_acc = evaluate()\n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} \"\n",
    "          f\"| test loss {te_loss} acc {te_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6cbf185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc2cc785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4383148",
   "metadata": {},
   "source": [
    "## 2.2 Data representation for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657dba32",
   "metadata": {},
   "source": [
    "### 2.2.1 Scaler (rank-0 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3837f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3386d",
   "metadata": {},
   "source": [
    "### 2.2.2 Vectors (rank-1 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "657398e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12,  3,  6, 14,  7]), 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([12,3,6,14,7])\n",
    "x, x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47158f11",
   "metadata": {},
   "source": [
    "### Matrics (rank02 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7bda937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[5, 78,2, 35, 0],\n",
    "              [6, 779, 3, 25, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f154c",
   "metadata": {},
   "source": [
    "### Rank-3 and higher-rank tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b948983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "            [6, 79, 3, 35, 1],\n",
    "            [7, 80, 4, 36, 2]],\n",
    "            [[5, 78, 2, 34, 0],\n",
    "            [6, 79, 3, 35, 1],\n",
    "            [7, 80, 4, 36, 2]],\n",
    "            [[5, 78, 2, 34, 0],\n",
    "            [6, 79, 3, 35, 1],\n",
    "            [7, 80, 4, 36, 2]]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d550742",
   "metadata": {},
   "source": [
    "### 2.2.5 Key atrributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f5509",
   "metadata": {},
   "source": [
    "A tensor is defined by threee key attritutes:\n",
    "* Number of axes *(rank)*--For insttance,a rank-3 tensor has three axes, and a matrix as two axes. This is also called the tensor's ndim in python libraries such as NumPy or TensorFlow or PyTorch.\n",
    "* Shape--Thes is a tuple of integers that describes how many dimensions the tensor has algong each axis. For instance, the previous matrix example has shape (3, 5), adn the rank-3 tensor example has shape(3,3,5), A vector has shape with a single element, such as (5,), whereas a scalar has an empty shape, ().\n",
    "* *Data type (usually called **dtype** in python libaries)* -- This is the type of the data ontained in the tensor; for instance, a tensor's type could be float16, float32, float 64, uint8,, and so on. In tensorFlow, you are also likely to come across string tensors.\n",
    "\n",
    "\n",
    "To make this more concrete. lets look back at the data we proccesed in MNIST example. First, we load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b03c50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db90829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)  # Happens if GPU was already initialized\n",
    "\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52515fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import test_util\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae26053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply():\n",
    "    x = tf.random.uniform((100000, 10000), dtype=tf.float32)\n",
    "    y = tf.random.uniform((10000, 100000), dtype=tf.float32)\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73973e69",
   "metadata": {},
   "source": [
    "### 2.2.6 Manipulatng tensors in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54a3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 20:28:07.992827: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [100000,10000] vs. [10000,100000]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} Incompatible shapes: [100000,10000] vs. [10000,100000] [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwith tf.device(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/GPU:0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m     multiply()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/12 month ai challange/deep_learning/.venv_tf/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/12 month ai challange/deep_learning/.venv_tf/lib/python3.11/site-packages/IPython/core/magics/execution.py:1226\u001b[39m, in \u001b[36mExecutionMagics.timeit\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m):\n\u001b[32m   1225\u001b[39m     number = \u001b[32m10\u001b[39m ** index\n\u001b[32m-> \u001b[39m\u001b[32m1226\u001b[39m     time_number = \u001b[43mtimer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time_number >= \u001b[32m0.2\u001b[39m:\n\u001b[32m   1228\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/12 month ai challange/deep_learning/.venv_tf/lib/python3.11/site-packages/IPython/core/magics/execution.py:184\u001b[39m, in \u001b[36mTimer.timeit\u001b[39m\u001b[34m(self, number)\u001b[39m\n\u001b[32m    182\u001b[39m gc.disable()\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     timing = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<magic-timeit>:2\u001b[39m, in \u001b[36minner\u001b[39m\u001b[34m(_it, _timer)\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mmultiply\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m x = tf.random.uniform((\u001b[32m100000\u001b[39m, \u001b[32m10000\u001b[39m), dtype=tf.float32)\n\u001b[32m      3\u001b[39m y = tf.random.uniform((\u001b[32m10000\u001b[39m, \u001b[32m100000\u001b[39m), dtype=tf.float32)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/12 month ai challange/deep_learning/.venv_tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/12 month ai challange/deep_learning/.venv_tf/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5983\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   5982\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m5983\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} Incompatible shapes: [100000,10000] vs. [10000,100000] [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "with tf.device(\"/GPU:0\"):\n",
    "     multiply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ae372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    x = tf.random.uniform((100000, 10000), dtype=tf.float32)\n",
    "    y = tf.random.uniform((10000, 100000), dtype=tf.float32)\n",
    "    c = tf.matmul(x,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c976b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Memory growth warning:\", e)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caefa1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2048, 2048])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.random.normal([2048, 2048])   # ~16 MB\n",
    "    b = tf.random.normal([2048, 2048])\n",
    "    c = tf.matmul(a, b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c709c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd4199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.9 ms ± 775 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.random.normal([4096, 4096])   # now feasible\n",
    "    b = tf.random.normal([4096, 4096])\n",
    "    c = tf.matmul(a, b)                  # result is float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88bd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.48 s ± 199 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    a = tf.random.normal([10000, 10000])\n",
    "    b = tf.random.normal([10000, 10000])\n",
    "    c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371b8d9",
   "metadata": {},
   "source": [
    "## Back to book 2.2.4\n",
    "### By tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6f32f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dc0430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154e670c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5302c1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, (60000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.ndim, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978eccb",
   "metadata": {},
   "source": [
    "### by pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a202a5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "DATA_DIR = 'data'\n",
    "train_ds = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transforms.ToTensor)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d24a7",
   "metadata": {},
   "source": [
    "**Note**: The train_ds is not a tensor. its a ***object*** that has two tensor inside:\n",
    "* *data*\n",
    "* *target*\n",
    "\n",
    "So train_ds work like a  wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35333c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85fe1427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf17182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([60000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.targets.ndim, train_ds.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81f3fd",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf35e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba45de",
   "metadata": {},
   "source": [
    "### Torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58af523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_ds.data[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f480f7",
   "metadata": {},
   "source": [
    "## 2.2.6 Manupulating tensors in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b0e61d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 28, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow\n",
    "my_slice = train_images[10:100]\n",
    "display(my_slice.shape)\n",
    "# torch\n",
    "my_slice_pt = train_ds.data[10:100]\n",
    "my_slice_pt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481226ed",
   "metadata": {},
   "source": [
    "### The notion of data batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45036b",
   "metadata": {},
   "source": [
    "In general, the first axis (axis 0, bacause indexing starts at 0) in all data sensors you will com across in deep learning will be the samples axis (sometimes called the samples dimension). In the MNIST example, samples are images of digits.\n",
    "\n",
    "In addition, deep learniing models don't process an entire dataset at onece; rather they break the data into small batches, Concretely. here's one batch of our MNIST digits, with a batch size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "970625f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---tensorflow---\n",
    "batch = train_images[:128]\n",
    "## then next batch\n",
    "batch = train_images[128:256]\n",
    "## and The nth batch:\n",
    "n = 3\n",
    "batch = train_images[128 * n:128*(n+1)]\n",
    "#---torch---\n",
    "batch = train_ds.data[:128]\n",
    "## tehen next batch\n",
    "batch = train_ds.data[128:256]\n",
    "## and the nth batch\n",
    "n = 3\n",
    "batch = train_ds.data[128*n : 128*(n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0142",
   "metadata": {},
   "source": [
    "## 2.2.8 Real-world example of data tensros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bd47e",
   "metadata": {},
   "source": [
    "The data you'll manipulate will almost always fall into following catagories: \n",
    "* ***vector data** *: Rank-2 tensors of shape (sample, fetures), where eatch sample is a vector of numerical attributes (\"features\")\n",
    "\n",
    "\n",
    "* ***Timeseries data*** : Rank-3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of freature vectors\n",
    "\n",
    "\n",
    "* ***Images*** : Rank-4 tensors of shape (sample, height, width, channels), where each samples is a 2D grid of pixels, and each pixel is represented by a vector of vlues (\"Channels\")\n",
    "\n",
    "\n",
    "* ***Video*** : Rank-5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805a731",
   "metadata": {},
   "source": [
    "## 2.2.9 Vector data\n",
    "This is one of the most common cases. In such a dataset, each single data point can be\n",
    "encoded as a vector, and thus a batch of data will be encoded as a rank-2 tensor (that\n",
    "is, an array of vectors), where the first axis is the samples axis and the second axis is the\n",
    "features axis.\n",
    "\n",
    "Let's take at two examples:\n",
    "* An actuarial dataset of people, where we consider each person’s age, gender,\n",
    "and income. Each person can be characterized as a vector of 3 values, and thus\n",
    "an entire dataset of 100,000 people can be stored in a rank-2 tensor of shape\n",
    "(100000, 3).\n",
    "\n",
    "* A dataset of text documents, where we represent each document by the counts\n",
    "of how many times each word appears in it (out of a dictionary of 20,000 common\n",
    "words). Each document can be encoded as a vector of 20,000 values (one\n",
    "count per word in the dictionary), and thus an entire dataset of 500 documents\n",
    "can be stored in a tensor of shape (500, 20000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460283a5",
   "metadata": {},
   "source": [
    "## 2.2.10 Timeseries data of sequence data\n",
    "Whenever time matters in your data (or the notion of sequence order), it makes sense\n",
    "to store it in a rank-3 tensor with an explicit time axis. Each sample can be encoded as\n",
    "a sequence of vectors (a rank-2 tensor), and thus a batch of data will be encoded as a\n",
    "rank-3 tensor.\n",
    "\n",
    "\n",
    "![sample image](images/1.png)\n",
    "\n",
    "\n",
    "<!-- <img src=\"images/1.png\" weight=\"300\"/> -->\n",
    "The time axis is always the second axis (axis of index 1) by convention. Let's look at a few examples:\n",
    "* A dataset of stock prices. Every minute. we store the current price of the stock the highest price in the past minute, and the lowest price in the past minute. Thus, every minute is encuded as 3D vector, as etire day of trading is ecoded as a matrix as a matrix of shape (390, 3) (there are 390 minutes in the trading data), and 250 days' worth of data can be stored in a rank-3 tensor of shape(250, 390,3). Here each sample would be one day's wordth of data\n",
    "* A dataset of tweets, were we encode each tweet as a sequence if 289 char out of an alphabet of 128 unique chars. In this setting, each character can be encoded as a binary vector of size 1(an all-zeroes vector exept for a 1 entry at the index correspondeing to the chacter). Then each tweet can be encoded as a rank-2 tensor of shape (289, 128). and a dataset of 1 million tweets can be encoded as a randk-2 tensor of shape (280, 128), and a dataset of 1 million tweets can be stored in a tensor of shape (1,000,000, 280, 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965592a",
   "metadata": {},
   "source": [
    "### 2.2.11 Image data\n",
    "Image typically have three dimensions: height, width and color depth. Although grayscale images (like our MNIST digts) have only a single color chanell and could thus be stored in randk 2 tensor. by convertion image tensors are always rank-3, with a one dimensional color cchannel for grayscale images. Ab atch of 128 grauscale images of size 256 x 256 coud thus be stored in a tensor of shape (128, 256, 256, 1), and a batch of 138 color images could be stored in a tensor of shape  (128, 256, 256, 3)\n",
    "\n",
    "\n",
    "![blablabla](images/2.png)\n",
    "\n",
    "***Note***:\n",
    "* Tensorflow standard: ```(sample, height, width, channels)```\n",
    "* Pytorch standard: ```(sameple, channels, height, width)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d0cf1",
   "metadata": {},
   "source": [
    "### 2.2.12 Video data\n",
    "Video data is one of the few types of real-world data for which you’ll need rank-5 tensors.\n",
    "A video can be understood as a sequence of frames, each frame being a color\n",
    "image. Because each frame can be stored in a rank-3 tensor (height, width, color_\n",
    "depth), a sequence of frames can be stored in a rank-4 tensor (frames, height,\n",
    "width, color_depth), and thus a batch of different videos can be stored in a rank-5\n",
    "tensor of shape (samples, frames, height, width, color_depth).\n",
    "For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\n",
    "second would have 240 frames. A batch of four such video clips would be stored in a\n",
    "tensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the dtype of the tensor was float32, each value would be stored in 32 bits, so the tensor\n",
    "would represent 405 MB. Heavy! Videos you encounter in real life are much lighter,\n",
    "because they aren’t stored in float32, and they’re typically compressed by a large factor\n",
    "(such as in the MPEG format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9fefe",
   "metadata": {},
   "source": [
    "## 2.3 The gears of nerual networks: Tensor operations\n",
    "A keras layer instance look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca4742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense, built=False>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "keras.layers.Dense (512, activation='relu')\n",
    "# output = relu(dot(input, W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6535f6a",
   "metadata": {},
   "source": [
    "In PyTorch dense is **nn.Linear** and you add **ReLU** separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9d16415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# Example input has 32 batch of 1000 feature\n",
    "x = torch.rand(32, 1000)\n",
    "# Dense laryer 1000->512\n",
    "layer = nn.Linear(1000, 512)\n",
    "relu = nn.ReLU()\n",
    "\n",
    "output = relu(layer(x))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a048c25",
   "metadata": {},
   "source": [
    "🧠 What actually happens\n",
    "\n",
    "\n",
    "If input is shaped [batch_size, 1000]:\n",
    "*\tMultiply by weight matrix W of shape [1000, 512].\n",
    "*\tAdd bias vector b of shape [512].\n",
    "*\tApply activation (e.g. ReLU).\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "\\text{output} = \\text{ReLU}(XW + b)\n",
    "*\tInput shape → [batch, 1000]\n",
    "*\tOutput shape → [batch, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c765c0",
   "metadata": {},
   "source": [
    "### 2.3.1 Element-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae18645",
   "metadata": {},
   "source": [
    "native python way relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "518bafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def native_relu(x):\n",
    "    assert len(x.shape) == 2 # x is a rank-2 NumPy tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i,j],0)\n",
    "    return x\n",
    "\n",
    "def native_add(x,y):\n",
    "    assert len(x.shape)==2\n",
    "    assert x.shape==y.shape\n",
    "    x = x.copy()\n",
    "    for i in range (x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]+=y[i,j]\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974b2cb",
   "metadata": {},
   "source": [
    "### 2.3.2 BroadCasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a616ed7",
   "metadata": {},
   "source": [
    "### 2.3.3 Tensor Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff9478",
   "metadata": {},
   "source": [
    "### 2.3.4 Tensor reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffb2a8",
   "metadata": {},
   "source": [
    "### 2.3.6 Visual representation of tensor manupulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616d179",
   "metadata": {},
   "source": [
    "## 2.4 *(very Important)* ***The engine of neural networkds: Gradient-based optimization***\n",
    "1. Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "2. Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "3. Compute the loss of the model on the batch, a measure of the mismatch between\n",
    "y_pred and y_true.\n",
    "4. Update all weights of the model in a way that slightly reduces the loss on this\n",
    "batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91816d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
